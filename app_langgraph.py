"""
ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ ì•± - LangGraph ë²„ì „
"""

import streamlit as st
import pandas as pd
from supabase import create_client
from openai import OpenAI
import os
from dotenv import load_dotenv
from datetime import datetime
import time
import json
import re
import requests
from bs4 import BeautifulSoup
import numpy as np

# LangGraph ê´€ë ¨ ì„í¬íŠ¸
from typing import TypedDict, Annotated, List, Union, Dict
from langgraph.graph import StateGraph, END
import operator

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ (LangGraph)",
    page_icon="ğŸ›’",
    layout="wide"
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# API í‚¤ ì„¤ì •
SUPABASE_URL = os.getenv("SUPABASE_URL") or st.secrets.get("SUPABASE_URL", "")
SUPABASE_KEY = os.getenv("SUPABASE_KEY") or st.secrets.get("SUPABASE_KEY", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY", "")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID") or st.secrets.get("NAVER_CLIENT_ID", "")
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") or st.secrets.get("NAVER_CLIENT_SECRET", "")

# CSS ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    .main-header {
        text-align: center;
        padding: 2rem 0;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .pros-section {
        background-color: #d4edda;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #28a745;
    }
    .cons-section {
        background-color: #f8d7da;
        padding: 1.5rem;
        border-radius: 8px;
        margin: 1rem 0;
        border-left: 5px solid #dc3545;
    }
    .process-info {
        background-color: #e3f2fd;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
        border-left: 3px solid #2196f3;
    }
</style>
""", unsafe_allow_html=True)

# í—¤ë”
st.markdown("""
<div class="main-header">
    <h1>ğŸ›’ ìŠ¤ë§ˆíŠ¸í•œ ì‡¼í•‘ (LangGraph Edition)</h1>
    <p style="font-size: 1.2rem; margin-top: 1rem;">
        LangGraphë¡œ êµ¬í˜„í•œ ì§€ëŠ¥í˜• ì œí’ˆ ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ
    </p>
</div>
""", unsafe_allow_html=True)

# ========================
# LangGraph State ì •ì˜
# ========================

class SearchState(TypedDict):
    """ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœ"""
    product_name: str
    search_method: str  # "database", "web_crawling", "similarity"
    results: Dict
    pros: List[str]
    cons: List[str]
    sources: List[Dict]
    messages: Annotated[List[str], operator.add]  # í”„ë¡œì„¸ìŠ¤ ë¡œê·¸
    error: str
    db_search_complete: bool
    web_search_complete: bool
    save_complete: bool

# ========================
# í—¬í¼ í´ë˜ìŠ¤ë“¤
# ========================

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
@st.cache_resource
def get_openai_client():
    return OpenAI(api_key=OPENAI_API_KEY)

@st.cache_resource
def get_supabase_client():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

# OpenAI ì„ë² ë”© í´ë˜ìŠ¤
class OpenAIEmbeddings:
    def __init__(self):
        self.client = get_openai_client()
        self.model = "text-embedding-ada-002"
    
    def get_embedding(self, text: str):
        try:
            response = self.client.embeddings.create(
                input=text,
                model=self.model
            )
            return response.data[0].embedding
        except Exception as e:
            st.error(f"ì„ë² ë”© ìƒì„± ì˜¤ë¥˜: {e}")
            return None
    
    def cosine_similarity(self, vec1, vec2):
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)

# í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
class NaverBlogCrawler:
    def __init__(self):
        self.headers = {
            "X-Naver-Client-Id": NAVER_CLIENT_ID,
            "X-Naver-Client-Secret": NAVER_CLIENT_SECRET
        }
    
    def search_blog(self, query, display=10):
        url = "https://openapi.naver.com/v1/search/blog"
        params = {"query": query, "display": display, "sort": "sim"}
        
        try:
            response = requests.get(url, headers=self.headers, params=params)
            if response.status_code == 200:
                return response.json()
        except Exception as e:
            st.error(f"ë¸”ë¡œê·¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return None
    
    def crawl_content(self, url):
        try:
            if "blog.naver.com" in url:
                parts = url.split('/')
                if len(parts) >= 5:
                    blog_id = parts[3]
                    post_no = parts[4].split('?')[0]
                    mobile_url = f"https://m.blog.naver.com/{blog_id}/{post_no}"
                    
                    response = requests.get(mobile_url, headers={
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    })
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        
                        for selector in ['div.se-main-container', 'div#postViewArea', 'div.post_ct']:
                            elem = soup.select_one(selector)
                            if elem:
                                content = elem.get_text(separator='\n', strip=True)
                                content = re.sub(r'\s+', ' ', content)
                                return content if len(content) > 300 else None
        except:
            pass
        return None

# ========================
# LangGraph ë…¸ë“œ í•¨ìˆ˜ë“¤
# ========================

def search_database_node(state: SearchState) -> SearchState:
    """ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë…¸ë“œ"""
    product_name = state["product_name"]
    state["messages"].append(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ '{product_name}' ê²€ìƒ‰ ì¤‘...")
    
    supabase = get_supabase_client()
    embeddings_helper = OpenAIEmbeddings()
    
    try:
        # 1. ì •í™•í•œ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").eq('product_name', product_name).execute()
        if result.data:
            state["search_method"] = "database"
            state["results"] = {"data": result.data}
            state["messages"].append(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ì œí’ˆ ë°œê²¬!")
            state["db_search_complete"] = True
            return state
        
        # 2. ë¶€ë¶„ ë§¤ì¹­
        result = supabase.table('laptop_pros_cons').select("*").ilike('product_name', f'%{product_name}%').execute()
        if result.data:
            state["search_method"] = "database"
            state["results"] = {"data": result.data}
            state["messages"].append(f"ğŸ“Œ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ì œí’ˆ ë°œê²¬!")
            state["db_search_complete"] = True
            return state
        
        # 3. ìœ ì‚¬ë„ ê²€ìƒ‰
        state["messages"].append(f"ğŸ¤– AI ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œë„ ì¤‘...")
        query_embedding = embeddings_helper.get_embedding(product_name)
        
        if query_embedding:
            all_products = supabase.table('laptop_pros_cons').select("*").execute()
            similar_products = []
            checked_products = set()
            
            for item in all_products.data:
                if item['product_name'] in checked_products:
                    continue
                
                if item.get('embedding'):
                    try:
                        item_embedding = json.loads(item['embedding']) if isinstance(item['embedding'], str) else item['embedding']
                        similarity = embeddings_helper.cosine_similarity(query_embedding, item_embedding)
                        
                        if similarity >= 0.7:
                            similar_products.append({
                                'product_name': item['product_name'],
                                'similarity': similarity
                            })
                            checked_products.add(item['product_name'])
                    except:
                        pass
            
            if similar_products:
                similar_products.sort(key=lambda x: x['similarity'], reverse=True)
                best_match = similar_products[0]['product_name']
                result = supabase.table('laptop_pros_cons').select("*").eq('product_name', best_match).execute()
                if result.data:
                    state["search_method"] = "similarity"
                    state["results"] = {"data": result.data}
                    state["messages"].append(f"ğŸ¯ AIê°€ ìœ ì‚¬í•œ ì œí’ˆ '{best_match}'ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
                    state["db_search_complete"] = True
                    return state
        
        state["messages"].append(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì œí’ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        state["results"] = {"data": None}
        state["db_search_complete"] = True
        
    except Exception as e:
        state["error"] = str(e)
        state["messages"].append(f"âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        state["db_search_complete"] = True
    
    return state

def crawl_web_node(state: SearchState) -> SearchState:
    """ì›¹ í¬ë¡¤ë§ ë…¸ë“œ"""
    if state["results"].get("data"):  # ì´ë¯¸ DBì—ì„œ ì°¾ì€ ê²½ìš°
        return state
    
    product_name = state["product_name"]
    state["search_method"] = "web_crawling"
    state["messages"].append(f"ğŸŒ ì›¹ì—ì„œ '{product_name}' ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘...")
    
    crawler = NaverBlogCrawler()
    openai_client = get_openai_client()
    
    all_pros = []
    all_cons = []
    sources = []
    
    search_queries = [
        f"{product_name} ì¥ë‹¨ì  ì‹¤ì‚¬ìš©",
        f"{product_name} í›„ê¸°"
    ]
    
    for query in search_queries[:2]:
        state["messages"].append(f"ğŸ” ê²€ìƒ‰ì–´: '{query}'")
        blog_result = crawler.search_blog(query, display=5)
        
        if not blog_result or 'items' not in blog_result:
            continue
        
        for post in blog_result['items'][:3]:
            title = BeautifulSoup(post['title'], "html.parser").get_text()
            state["messages"].append(f"ğŸ“– ë¶„ì„ ì¤‘: {title[:30]}...")
            
            content = crawler.crawl_content(post['link'])
            
            if content:
                # GPTë¡œ ì¥ë‹¨ì  ì¶”ì¶œ
                prompt = f"""ë‹¤ìŒì€ "{product_name}"ì— ëŒ€í•œ ë¸”ë¡œê·¸ ë¦¬ë·°ì…ë‹ˆë‹¤.

[ë¸”ë¡œê·¸ ë‚´ìš©]
{content[:1500]}

ìœ„ ë‚´ìš©ì—ì„œ {product_name}ì˜ ì¥ì ê³¼ ë‹¨ì ì„ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì¥ì :
- (êµ¬ì²´ì ì¸ ì¥ì )

ë‹¨ì :
- (êµ¬ì²´ì ì¸ ë‹¨ì )

ì •ë³´ê°€ ë¶€ì¡±í•˜ë©´ "ì •ë³´ ë¶€ì¡±"ì´ë¼ê³  ë‹µí•˜ì„¸ìš”."""
                
                try:
                    response = openai_client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "ë…¸íŠ¸ë¶ ë¦¬ë·° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                            {"role": "user", "content": prompt}
                        ],
                        temperature=0.3,
                        max_tokens=500
                    )
                    
                    result = response.choices[0].message.content.strip()
                    
                    if result and "ì •ë³´ ë¶€ì¡±" not in result:
                        pros = []
                        cons = []
                        
                        lines = result.split('\n')
                        current_section = None
                        
                        for line in lines:
                            line = line.strip()
                            if 'ì¥ì :' in line:
                                current_section = 'pros'
                            elif 'ë‹¨ì :' in line:
                                current_section = 'cons'
                            elif line.startswith('-') and current_section:
                                point = line[1:].strip()
                                if point and len(point) > 5:
                                    if current_section == 'pros':
                                        pros.append(point)
                                    else:
                                        cons.append(point)
                        
                        if pros or cons:
                            all_pros.extend(pros)
                            all_cons.extend(cons)
                            sources.append({
                                'title': title,
                                'link': post['link']
                            })
                            state["messages"].append(f"âœ“ ì¥ì  {len(pros)}ê°œ, ë‹¨ì  {len(cons)}ê°œ ì¶”ì¶œ")
                except:
                    pass
            
            time.sleep(0.5)
    
    # ì¤‘ë³µ ì œê±°
    state["pros"] = list(dict.fromkeys(all_pros))[:10]
    state["cons"] = list(dict.fromkeys(all_cons))[:10]
    state["sources"] = sources[:5]
    
    state["messages"].append(f"ğŸ‰ ì›¹ í¬ë¡¤ë§ ì™„ë£Œ! ì´ ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ ìˆ˜ì§‘")
    state["web_search_complete"] = True
    
    return state

def process_results_node(state: SearchState) -> SearchState:
    """ê²°ê³¼ ì²˜ë¦¬ ë…¸ë“œ"""
    if state["search_method"] in ["database", "similarity"] and state["results"].get("data"):
        # DB ê²°ê³¼ ì²˜ë¦¬
        data = state["results"]["data"]
        state["pros"] = [item['content'] for item in data if item['type'] == 'pro']
        state["cons"] = [item['content'] for item in data if item['type'] == 'con']
        state["sources"] = []
        
        state["messages"].append(f"ğŸ“‹ ê²°ê³¼ ì •ë¦¬: ì¥ì  {len(state['pros'])}ê°œ, ë‹¨ì  {len(state['cons'])}ê°œ")
    
    return state

def save_to_database_node(state: SearchState) -> SearchState:
    """ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë…¸ë“œ"""
    if state["search_method"] != "web_crawling" or not (state["pros"] or state["cons"]):
        return state
    
    state["messages"].append(f"ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
    
    supabase = get_supabase_client()
    embeddings_helper = OpenAIEmbeddings()
    
    try:
        # ì„ë² ë”© ìƒì„± (ì„ íƒì )
        embedding = None
        # embedding = embeddings_helper.get_embedding(state["product_name"])
        
        data = []
        for pro in state["pros"]:
            data.append({
                'product_name': state["product_name"],
                'type': 'pro',
                'content': pro,
                # 'embedding': embedding
            })
        
        for con in state["cons"]:
            data.append({
                'product_name': state["product_name"],
                'type': 'con',
                'content': con,
                # 'embedding': embedding
            })
        
        if data:
            supabase.table('laptop_pros_cons').insert(data).execute()
            state["messages"].append(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            state["save_complete"] = True
    except Exception as e:
        state["messages"].append(f"âš ï¸ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
    
    return state

# ========================
# LangGraph ì›Œí¬í”Œë¡œìš°
# ========================

def should_search_web(state: SearchState) -> str:
    """ì›¹ ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ ê²°ì •"""
    if state["results"].get("data"):
        return "process"
    else:
        return "crawl"

def should_save_to_db(state: SearchState) -> str:
    """DB ì €ì¥ í•„ìš” ì—¬ë¶€ ê²°ì •"""
    if state["search_method"] == "web_crawling" and (state["pros"] or state["cons"]):
        return "save"
    else:
        return "end"

@st.cache_resource
def create_search_workflow():
    """ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    workflow = StateGraph(SearchState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("search_db", search_database_node)
    workflow.add_node("crawl_web", crawl_web_node)
    workflow.add_node("process", process_results_node)
    workflow.add_node("save_db", save_to_database_node)
    
    # ì—£ì§€ ì„¤ì •
    workflow.set_entry_point("search_db")
    
    # DB ê²€ìƒ‰ í›„ ë¶„ê¸°
    workflow.add_conditional_edges(
        "search_db",
        should_search_web,
        {
            "crawl": "crawl_web",
            "process": "process"
        }
    )
    
    # ì›¹ í¬ë¡¤ë§ í›„ ì²˜ë¦¬
    workflow.add_edge("crawl_web", "process")
    
    # ê²°ê³¼ ì²˜ë¦¬ í›„ ë¶„ê¸°
    workflow.add_conditional_edges(
        "process",
        should_save_to_db,
        {
            "save": "save_db",
            "end": END
        }
    )
    
    # DB ì €ì¥ í›„ ì¢…ë£Œ
    workflow.add_edge("save_db", END)
    
    return workflow.compile()

# ========================
# Streamlit UI
# ========================

# ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”
search_app = create_search_workflow()

# ê²€ìƒ‰ UI
col1, col2, col3 = st.columns([1, 3, 1])

with col2:
    product_name = st.text_input(
        "ğŸ” ì œí’ˆëª…ì„ ì…ë ¥í•˜ì„¸ìš”",
        placeholder="ì˜ˆ: ë§¥ë¶ í”„ë¡œ M3, LG ê·¸ë¨ 2024, ê°¤ëŸ­ì‹œë¶4 í”„ë¡œ"
    )
    
    col_btn1, col_btn2 = st.columns(2)
    with col_btn1:
        search_button = st.button("ğŸ” ê²€ìƒ‰í•˜ê¸°", use_container_width=True, type="primary")
    with col_btn2:
        show_process = st.checkbox("ğŸ”§ í”„ë¡œì„¸ìŠ¤ ë³´ê¸°", value=False)

# ê²€ìƒ‰ ì‹¤í–‰
if search_button and product_name:
    # ì´ˆê¸° ìƒíƒœ
    initial_state = {
        "product_name": product_name,
        "search_method": "",
        "results": {},
        "pros": [],
        "cons": [],
        "sources": [],
        "messages": [],
        "error": "",
        "db_search_complete": False,
        "web_search_complete": False,
        "save_complete": False
    }
    
    # LangGraph ì‹¤í–‰
    with st.spinner(f"'{product_name}' ê²€ìƒ‰ ì¤‘..."):
        final_state = search_app.invoke(initial_state)
    
    # í”„ë¡œì„¸ìŠ¤ ë¡œê·¸ í‘œì‹œ
    if show_process and final_state["messages"]:
        with st.expander("ğŸ”§ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤", expanded=True):
            for msg in final_state["messages"]:
                st.write(msg)
    
    # ê²°ê³¼ í‘œì‹œ
    if final_state["pros"] or final_state["cons"]:
        # ê²€ìƒ‰ ì •ë³´
        st.markdown(f"""
        <div class="process-info">
            <strong>ê²€ìƒ‰ ë°©ë²•:</strong> {
                'ë°ì´í„°ë² ì´ìŠ¤ (ì •í™•íˆ ì¼ì¹˜)' if final_state["search_method"] == "database" else
                'AI ìœ ì‚¬ë„ ê²€ìƒ‰' if final_state["search_method"] == "similarity" else
                'ì›¹ í¬ë¡¤ë§'
            } | 
            <strong>ì¥ì :</strong> {len(final_state["pros"])}ê°œ | 
            <strong>ë‹¨ì :</strong> {len(final_state["cons"])}ê°œ
        </div>
        """, unsafe_allow_html=True)
        
        # ì¥ë‹¨ì  í‘œì‹œ
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            <div class="pros-section">
                <h3>âœ… ì¥ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, pro in enumerate(final_state["pros"], 1):
                st.write(f"{idx}. {pro}")
        
        with col2:
            st.markdown("""
            <div class="cons-section">
                <h3>âŒ ë‹¨ì </h3>
            </div>
            """, unsafe_allow_html=True)
            
            for idx, con in enumerate(final_state["cons"], 1):
                st.write(f"{idx}. {con}")
        
        # ì¶œì²˜ (ì›¹ í¬ë¡¤ë§ì¸ ê²½ìš°)
        if final_state["sources"]:
            with st.expander("ğŸ“š ì¶œì²˜ ë³´ê¸°"):
                for idx, source in enumerate(final_state["sources"], 1):
                    st.write(f"{idx}. [{source['title']}]({source['link']})")
        
        # í†µê³„
        st.markdown("---")
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ ì¥ì ", f"{len(final_state['pros'])}ê°œ")
        with col2:
            st.metric("ì´ ë‹¨ì ", f"{len(final_state['cons'])}ê°œ")
        with col3:
            st.metric("ê²€ìƒ‰ ë°©ë²•", 
                     "DB" if final_state["search_method"] == "database" else 
                     "AI" if final_state["search_method"] == "similarity" else 
                     "ì›¹")
        with col4:
            st.metric("í”„ë¡œì„¸ìŠ¤ ë‹¨ê³„", 
                     f"{len(final_state['messages'])}ê°œ")
    else:
        st.error(f"'{product_name}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

# í•˜ë‹¨ ì •ë³´
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    st.info("ğŸ’¡ LangGraphë¡œ êµ¬í˜„ëœ ì²´ê³„ì ì¸ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤")
with col2:
    st.info("ğŸ¤– OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•œ ì§€ëŠ¥í˜• ê²€ìƒ‰")
with col3:
    st.info("ğŸ“Š ìë™ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ë° ê´€ë¦¬")

current_date = datetime.now().strftime('%Yë…„ %mì›” %dì¼')
st.markdown(f"""
<div style="text-align: center; color: #666; padding: 2rem;">
    <p>ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {current_date}</p>
    <p>Powered by LangGraph & OpenAI</p>
</div>
""", unsafe_allow_html=True)
